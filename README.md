# Interactive-Project

First version requires input from the ConvnetOsc module (1000 inputs).

Final version requires image-net-2012.sqlite3 to be blaced in the bin/data/ folder. The file can be downloaded [here](https://raw.githubusercontent.com/liuliu/ccv/unstable/samples/image-net-2012.sqlite3).
# Original Idea

The original idea for this project was to use a smartphone camera to recognize touch gestures on a non-touch screen. The motivation for this was that certain gestures are easier to perform on a touch screen compared to a trackpad or keyboard. For example, signing a pdf document or scrolling through an ebook may feel more natural by hand rather than through the trackpad. Furthermore, it may be possible to define three-dimensional gestures that do not touch the screen. For example, swiping up with four fingers on a mac screen may open up mission control, but swiping up four fingers in the air in front of the screen may raise the volume instead. While these gestures could possibly be useful, the main aim was still to achieve touch gestures on a non-touch screen.

Although the project seemed difficult, it still seamed feasbile. If a smartphone was aligned with a screen in such a way that the line of sight of the camera was along the plane of the screen, then it would be possible to recognize when the screen had been touched. Furthermore, as the hand grew closer or further away, its size would change from the perspective of the camera, thereby allowing us to determine its position along the x-axis of the screen. As the hand went up or down, it would also go up or down from the perspective of the camera, therefore allowing us to determine the position along the y-axis.

# Prototype Idea

After beginning the project, a few problems immediately appeared. The first was that sending a live stream from a smartphone to a computer was slow and difficult. Second was that the smartphone would have nothing to keep it stable, meaning that anyone who wanted to use this program would have to have a dock or some form of holder. To resolve these two difficulties, I switched from using the smartphone to using the laptop camera, and had the camera pointed at a computer monitor. This ensured that the camera was stable, and that the live stream would not need to be transferred externally. For people who use their laptops in a kind of 'workstation' setup, with a keyboard and monitor and other peripherals connected, the laptop might not actually be used directly. In this kind of setup, using the laptop instead of the smartphone may actually be desired.

Another problem was that the camera stream had to be transformed before going into a classifier. If the camera stream was used directly, even if it worked, there would exist a certain boundary beyond which none of the training data would pass. When training, all the 'touch' gestures would be along the boundary, and non-touch gestures would be behind. There would be no training sets where the hand passes through the computer monitor screen. What this means is that a boundary would be created such that anything before or after that boundary would not be recognized as a touch gesture. Thus, if the laptop were to be shifted slightly, none of the gestures would be recognized as touch gestures anymore.

When testing different training models, I tried sending the video stream into a cnn, and the output of the cnn (the last layer) into a classifier. I am not completely sure as to why this worked, but it is possible that the cnn was recognizing the 'distorted' hands as different objects. When the distortion was similar, such as when pressing the same button, it would be recognized as the same by the cnn again. While not completely accurate, it was accurate enough for the occasional use, recognizing about 80-90% of the touches, and having very few false positives when the hand was outside the range of the camera. However, it was still highly dependent on the background. During the user testing, I found that I had to re-train the classifier in the new environment. The accuracy seemed to have dropped with this new background, though part of the drop can also be attributed to the fewer training samples used. Furthermore, when the background changed, such as when a person appeared in the background, the accuracy would drop significantly. Thus, there were not too many advantages in using this version.

[User Test 1](https://youtu.be/zebaHpginQI), [User Test 2](https://youtu.be/hMMofna8Z3g), [User Test 3](https://youtu.be/jvUD_hfI7rI)

# Final Version

To reduce the effect of the background on the accuracy of the model, I decided to use background subtraction to reduce the excess pixels. However, this also removed the monitor from the frame. Without the monitor, the same problem with the 'boundary' would occur again, since it would simply appear as if the hand were reaching a certain point and stopping. In order to counter this problem, I was thinking that the boundary of the monitor could be found and excluded from the background subtraction. While this might not be true for all monitors, most monitors have flat screens. In addition, the camera would be located extremely close to the monitor. As such, it might be possible to look at the straight edges and determine which set belongs to the monitor. EVen if the exact boundary is not found, an approximation would be sufficient.

While examining an edge detection algorithm in order to locate the monitor, I noticed that the algorithm would even identify reflections on the monitor screen. Since the reflection was not a part of the original background and only appeared when the finger was close to the monitor, it would appear in the image even after the background subtraction. By using the fact that a reflection appeared, and its distance to the hand, as an indicator that a touch occured, it should be possible to make the accuracy of the model independent of the device location. A hand no longer has to cross a certain 'boundary' in order for a touch to occur- rather, it should be close to its reflection for the event to be registered as a touch. While this method may still have flaws, for example, the existence of a reflection does not necessarily mean that a touch occured, and it may not work for all displays and lighting conditions, I thought it was still worth testing to get an idea for its accuracy.

Unfortunately, while I was able to combine the edge detection with the background reduction, I was not able to send the final image to Wekinator as input due to its size. I tried to send it into a cnn before sending it to Wekinator, since that reduced the input size to 4096. However, the results for this were extremely inaccurate and unpredictable. I also tried reducing the size of the file by taking every ith pixel (for a fixed value i, e.g. 100), however, in this case the model failed to register touch events at all, possibly because too much information had been lost in the downsampling.

While I could not finish this version in time for the final presentations, I was able to get some ideas for what the next steps should be. One thing that could be added is a calibration tool, such as a physical accessory or software 'straightedge', that could potentially be used to calculate the angle with the screen. This could be used to essentially normalize the touch events to determine where they occured. Furthermore, an 'average' downsampling could be used to reduce the size of the image. Whether this average is computationally feasible remains to be seen, but the existence of certain events, such as the appearance of reflections, should be reflected in the 'average' downsample while they were not in the previous downsampling method. Alternatives to wekinator, such as Dlib, could be used so that data does not have to be passed through osc, thereby improving the performance.
